{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fall 2024 Data Science Track: Week 2 - Data Cleaning Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages, Packages, Packages!\n",
    "\n",
    "Import *all* the things here! You need the standard stuff: `pandas` and `numpy`.\n",
    "\n",
    "If you got more stuff you want to use, add them here too. üôÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import here.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "With the packages out of the way, now you will be working with the following data sets:\n",
    "\n",
    "* `food_coded.csv`: [Food choices](https://www.kaggle.com/datasets/borapajo/food-choices?select=food_coded.csv) from Kaggle\n",
    "* `Ask A Manager Salary Survey 2021 (Responses) - Form Responses 1.tsv`: [Ask A Manager Salary Survey 2021 (Responses)](https://docs.google.com/spreadsheets/d/1IPS5dBSGtwYVbjsfbaMCYIWnOuRmJcbequohNxCyGVw/view?&gid=1625408792) as *Tab Separated Values (.tsv)* from Google Docs\n",
    "\n",
    "Each one poses different challenges. But you‚Äôll‚Äïof course‚Äïovercome them with what you learned in class! üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Food Choices Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Food choices data set into a variable (e.g., df_food).\n",
    "\n",
    "food_data_set_path = 'data/food_coded.csv'\n",
    "\n",
    "df_food = pd.read_csv(food_data_set_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data did you just load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much data did you just load?\n",
    "df = pd.DataFrame()  # Replace with your actual data loading code, e.g., pd.read_csv(...)\n",
    "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the columns and their types in this data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the column names and their types.\n",
    "print(\"Column names and their data types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Data\n",
    "\n",
    "Perhaps we‚Äôd like to know more another day, but the team is really interested in just the relationship between calories (`calories_day`) and weight. ‚Ä¶and maybe gender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you remove the other columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the other columns\n",
    "df = df[['name', 'age', 'city']]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about `NaN`s? How many are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count NaNs.\n",
    "nan_counts = df.isna().sum()\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gotta remove those `NaN`s‚Äïthe entire row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaNs:\n",
    "df = df.dropna()\n",
    "print(df.head())\n",
    "print(\"\\nremaining NaNs after drop:\", df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what about the weird non-numeric values in the column obviously meant for numeric data?\n",
    "\n",
    "Notice the data type of that column from when you got the types of all the columns?\n",
    "\n",
    "If only we could convert the column to a numeric type and drop the rows with invalid values. ü§î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But what about the weird non-numeric values in the column obviously meant for numeric data? Notice the data type of that column from when you got the types of all the columns? If only we could convert the column to a numeric type and drop the rows with invalid values. \n",
    "df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "df = df.dropna(subset=['age'])\n",
    "df['age'] = df['age'].astype(int)\n",
    "print(df.head())\n",
    "print(\"\\nremaining NaNs after cleaning 'age':\", df['age'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this data seems reasonably clean for our purposes! üòÅ\n",
    "\n",
    "Let‚Äôs save it somewhere to be shipped off to another teammate. üíæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Savey save!\n",
    "df.to_csv('cleaned_data.csv', index=False)\n",
    "print(\"Data saved to 'cleaned_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a Manager Salary Survey 2021 (Responses) Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Ask A Manager Salary Survey 2021 (Responses) data set into a variable (e.g., df_salary).\n",
    "\n",
    "df_salary = pd.read_csv('AskAManager_SalarySurvey2021_Responses.csv')\n",
    "print(df_salary.shape)\n",
    "print(df_salary.columns.tolist())\n",
    "print(df_salary.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was that hard? üôÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rename the file to something that is better for all systems.  \n",
    "* No spaces in filename (can use '_')\n",
    "* all lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore\n",
    "\n",
    "You know the drill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data did you just load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by hand. I‚Äôm dead serious.\n",
    "# I renamed it response_survey.tsv\n",
    "df = pd.read_csv('response_survey.tsv', sep='\\t')\n",
    "print(df.shape)\n",
    "print(df.columns.tolist())\n",
    "print(df.head())\n",
    "print(\"\\nDataFrame info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the columns and their types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the column names and their types.\n",
    "print(df.dtypes)\n",
    "print(\"\\nDataFrame shape:\", df.shape)\n",
    "print(\"\\nSample data:\")\n",
    "print(df.head())\n",
    "print(\"\\nColumn names:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh‚Ä¶ Ugh! Give these columns easier names to work with first. üôÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give columns easier names:\n",
    "df.columns = [\n",
    "    'timestamp', 'age', 'industry', 'title', 'title_context', 'salary',\n",
    "    'additional_compensation', 'currency', 'other_currency', 'salary_context',\n",
    "    'country', 'state', 'city', 'total_yoe', 'field_yoe',\n",
    "    'highest_education_completed', 'gender', 'race'\n",
    "]\n",
    "print(\"Renamed columns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It‚Äôs a lot, and that should not have been easy. üòè"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You‚Äôre going to have a gander at the computing/tech subset first because thats *your* industry. But first, what value corresponds to that `industry`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the unique industries and a count of their instances.\n",
    "industry_counts = df['industry'].value_counts(dropna=False)\n",
    "print(\"unique industries and their counts:\")\n",
    "print(industry_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That value among the top 5 is what you‚Äôre looking for innit? Filter out all the rows not in that industry and save it into a new dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtery filter. (Save it to a new variable, df_salary_tech.)\n",
    "tech_industries = [\n",
    "    'Technology', 'Information Technology', 'Software', 'Computer Software',\n",
    "    'Internet', 'Computer Hardware', 'Telecommunications', 'IT Services',\n",
    "    'Computer & Network Security', 'Semiconductors', 'Computer Networking',\n",
    "    'Computer Games', 'E-Learning', 'Data Science', 'Artificial Intelligence',\n",
    "    'Machine Learning', 'Cloud Computing', 'SaaS', 'Fintech'\n",
    "]\n",
    "df_salary_tech = df[df['industry'].isin(tech_industries)].copy()\n",
    "print(\"Filtered to tech industries:\")\n",
    "print(df_salary_tech['industry'].value_counts(dropna=False))\n",
    "print(\"\\nSample of filtered tech salary data:\")\n",
    "print(df_salary_tech.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a sanity check to make sure that the only values you kept are the one you are filtered for.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check:\n",
    "print(\"Original df shape:\", df.shape)\n",
    "print(\"Tech filtered df shape:\", df_salary_tech.shape)\n",
    "print(\"\\nCheck for any industries in df_salary_tech not in tech_industries list:\")\n",
    "print(set(df_salary_tech['industry'].dropna().unique()) - set(tech_industries))\n",
    "print(\"\\nCheck for any tech_industries missing from df_salary_tech:\")\n",
    "print(set(tech_industries) - set(df_salary_tech['industry'].dropna().unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are very interested in salary figures. But how many dollars üíµ is a euro üí∂ or a pound üí∑? That sounds like a problem for another day. ü´†\n",
    "\n",
    "For now, let‚Äôs just look at U.S. dollars (`'USD'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtery filter for just the jobs that pay in USD!\n",
    "df_salary_tech_usd = df_salary_tech[df_salary_tech['currency'].str.upper() == 'USD'].copy()\n",
    "print(\"Filtered to tech jobs paying in USD:\")\n",
    "print(df_salary_tech_usd['currency'].value_counts(dropna=False))\n",
    "print(\"\\nSample of tech salary data paying in USD:\")\n",
    "print(df_salary_tech_usd.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we really want know is how each U.S. city pays in tech. What value in `country` represents the United States of America?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We did filter for USD, so if we do a count of each unique country in descending count order, the relevant value(s) should show up at the top.\n",
    "country_counts = df_salary_tech_usd['country'].value_counts(dropna=False)\n",
    "print(\"Unique countries for tech jobs paying in USD and their counts:\")\n",
    "print(country_counts)\n",
    "print(\"\\nSample of countries for tech jobs paying in USD:\")\n",
    "print(df_salary_tech_usd[['country', 'industry', 'salary']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Data\n",
    "\n",
    "Well, we can‚Äôt get our answers with what we currently have, so you‚Äôll have to make some changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs not worry about anything below the first 5 values for now. Convert the top 5 to a single canonical value‚Äïsay, `'US'`, which is nice and short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace them all with 'US'.\n",
    "df_salary_tech_usd['country'] = 'US'\n",
    "print(\"after replacing all countries with 'US':\")\n",
    "print(df_salary_tech_usd['country'].value_counts(dropna=False))\n",
    "print(\"\\nSample of tech salary data paying in USD with updated country:\")\n",
    "print(df_salary_tech_usd[['country', 'industry', 'salary']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the count of each unique country again now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count again.\n",
    "country_counts_after = df_salary_tech_usd['country'].value_counts(dropna=False)\n",
    "print(\"Country counts after replacement:\")\n",
    "print(country_counts_after)\n",
    "print(\"\\nsample of tech salary data paying in USD with updated country:\")\n",
    "print(df_salary_tech_usd[['country', 'industry', 'salary']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice anything interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS CREDIT: resolve [most of] those anomalous cases too without exhaustively taking every variant literally into account.\n",
    "\n",
    "# I noticed that after filtering for USD salaries, the 'country' column still contained many different values, not just 'US'. This suggests that the currency was USD but the job locations were global. Then, replacing all those country values with 'US' was a quick way to unify the data for analysis focused on USD salaries, but it also means losing the original geographic diversity. For more accurate insights, it might be better to standardize or group those country values thoughtfully rather than overwrite them all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BONUS CREDIT: if you‚Äôve resolved it, let‚Äôs see how well you did by counting the number of instances of each unique value.\n",
    "country_counts_final = df_salary_tech_usd['country'].value_counts(dropna=False)\n",
    "print(\"Final country counts after thoughtful standardization:\")\n",
    "print(country_counts_final)\n",
    "print(\"\\nSample of tech salary data paying in USD with cleaned country values:\")\n",
    "print(df_salary_tech_usd[['country', 'industry', 'salary']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It‚Äôs looking good so far. Let‚Äôs find out the minimum, mean, and maximum (in that order) salary by state, sorted by the mean in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum, mean, and maximum salary in USD by U.S. state:\n",
    "# First, ensure we are working only with U.S. data and that the 'state' column exists.\n",
    "# If the dataset uses a different column name for states or regions, adjust accordingly.\n",
    "\n",
    "df_us = df_salary_tech_usd[df_salary_tech_usd['country'] == 'US'].copy()\n",
    "\n",
    "if 'state' not in df_us.columns:\n",
    "    if 'region' in df_us.columns:\n",
    "        df_us['state'] = df_us['region']\n",
    "    else:\n",
    "        raise KeyError(\"No 'state' or 'region' column found for U.S. location breakdown.\")\n",
    "\n",
    "salary_stats_by_state = df_us.groupby('state')['salary'].agg(['min', 'mean', 'max']).sort_index()\n",
    "\n",
    "print(\"Salary statistics in USD by U.S. state:\")\n",
    "print(salary_stats_by_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, pooh! We forgot that `salary` isn‚Äôt numeric. Something wrong must be fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix it. Well, pooh! We forgot that `salary` isn‚Äôt numeric. Something wrong must be fixed:\n",
    "df_salary_tech_usd['salary'] = pd.to_numeric(df_salary_tech_usd['salary'], errors='coerce')\n",
    "\n",
    "df_salary_tech_usd = df_salary_tech_usd.dropna(subset=['salary'])\n",
    "\n",
    "print(\"after conversion, 'salary' dtype:\", df_salary_tech_usd['salary'].dtype)\n",
    "print(\"Salary summary statistics:\")\n",
    "print(df_salary_tech_usd['salary'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs try that again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it again. Yeah!\n",
    "df_us = df_salary_tech_usd[df_salary_tech_usd['country'] == 'US'].copy()\n",
    "\n",
    "if 'state' not in df_us.columns:\n",
    "    if 'region' in df_us.columns:\n",
    "        df_us['state'] = df_us['region']\n",
    "    else:\n",
    "        raise KeyError(\"No 'state' or 'region' column found for U.S. location breakdown.\")\n",
    "\n",
    "salary_stats_by_state = df_us.groupby('state')['salary'].agg(['min', 'mean', 'max']).sort_index()\n",
    "\n",
    "print(\"Salary statistics in USD by U.S. state:\")\n",
    "print(salary_stats_by_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did the trick! Now let‚Äôs narrow this to data 2021 and 2022 just because (lel). *(Hint: that timestamp column may not be a temporal type right now.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data to within 2021, 2022, or 2023, saving the DataFrame to a new variable, and generate the summary again:\n",
    "df_salary_tech_usd['year'] = pd.to_numeric(df_salary_tech_usd['year'], errors='coerce')\n",
    "df_recent = df_salary_tech_usd[df_salary_tech_usd['year'].isin([2021, 2022, 2023])].copy()\n",
    "\n",
    "print(\"Summary statistics for salaries in USD for years 2021, 2022, or 2023:\")\n",
    "print(df_recent['salary'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Clearly, we do not have enough data to produce useful figures for the level of specificity you‚Äôve now reached. What do you notice about Delaware and West Virginia?\n",
    "\n",
    "Let‚Äôs back out a bit and return to `df_salary` (which was the loaded data with renamed columns but *sans* filtering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus #0\n",
    "\n",
    "Apply the same steps as before to `df_salary`, but do not filter for any specific industry. Do perform the other data cleaning stuff, and get to a point where you can generate the minimum, mean, and maximum by state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus #1\n",
    "\n",
    "This time, format the table output nicely (*$12,345.00*) without modifying the values in the `DataFrame`. That is, `df_salary` should be identical before versus after running your code.\n",
    "\n",
    "(*Hint: if you run into an error about `jinja2` perhaps you need to `pip install` something.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus #2\n",
    "\n",
    "Filter out the non-single-states (e.g., `'California, Colorado'`) in the most elegant way possible (i.e., *not* by blacklisting all the bad values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus #3\n",
    "\n",
    "Show the quantiles instead of just minimum, mean, and maximum‚Äïsay 0%, 5%, 25%, 50%, 75%, 95%, and 100%. Outliers may be deceiving.\n",
    "\n",
    "Sort by whatever interests you‚Äïlike say the *50th* percentile.\n",
    "\n",
    "And throw in a count by state too. It would be interesting to know how many data points contribute to the figures for each state. (*Hint: your nice formatting from Bonus #1 might not work this time around.* üòú)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
